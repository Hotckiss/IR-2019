{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import parallel_bulk\n",
    "import os\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import time\n",
    "from lxml import etree\n",
    "from sklearn.metrics import r2_score\n",
    "from datetime import timedelta\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_es_action(index, doc_id, document):\n",
    "    return {\n",
    "        '_index': index,\n",
    "        '_id': doc_id,\n",
    "        '_source': document\n",
    "    }\n",
    "\n",
    "def pretty_print_result(search_result, fields=None):\n",
    "    if fields is None:\n",
    "        fields = []\n",
    "    res = search_result['hits']\n",
    "    print(f'Total documents: {res[\"total\"][\"value\"]}')\n",
    "    for hit in res['hits']:\n",
    "        print(f'Doc {hit[\"_id\"]}, score is {hit[\"_score\"]}')\n",
    "        for field in fields:\n",
    "            print(f'{field}: {hit[\"_source\"][field]}')\n",
    "\n",
    "\n",
    "def get_score(search_result):\n",
    "    res = []\n",
    "    for hit in search_result['hits']['hits']:\n",
    "        res.append((hit[\"_id\"], hit[\"_score\"]))\n",
    "    res.sort(key = lambda x: x[1], reverse = True)\n",
    "    return res\n",
    "\n",
    "\n",
    "class Index:\n",
    "    def __init__(self, index, settings):\n",
    "        self.index_name = index\n",
    "        self.settings = settings\n",
    "        self.es = Elasticsearch([{'host': 'localhost', 'port': 9200, 'timeout': 360}])\n",
    "        if self.es.indices.exists(index=index):\n",
    "            self.es.indices.delete(index=index)\n",
    "        self.es.indices.create(index=index, body=settings)\n",
    "\n",
    "    def es_actions_generator(self, path_to_docs):\n",
    "        for doc_name in tqdm(os.listdir(path_to_docs)):\n",
    "            with open(f\"{path_to_docs}/{doc_name}\", \"r\", encoding=\"utf-8\") as inf:\n",
    "                doc_id = int(''.join(list(filter(str.isdigit, doc_name))))\n",
    "                doc = json.load(inf)           \n",
    "            yield create_es_action(self.index_name, doc_id, doc)\n",
    "\n",
    "\n",
    "    def add_documents(self, path_to_docs):\n",
    "        try:\n",
    "            for ok, result in parallel_bulk(self.es, self.es_actions_generator(path_to_docs), queue_size=4, thread_count=4,\n",
    "                                        chunk_size=1000):\n",
    "                  if not ok:\n",
    "                     print(result)\n",
    "        except Exception as e: \n",
    "            print(e)\n",
    "\n",
    "    def get_doc_by_id(self, doc_id):\n",
    "        return self.es.get(index=self.index_name, id=doc_id)['_source']\n",
    "\n",
    "    def search(self, query, *args):\n",
    "        return self.es.search(index=self.index_name, body=query, size=20)\n",
    "        # note that size set to 20 just because default value is 10 and we know that we have 12 docs and 10 < 12 < 20\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings_1 = {\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"text\": {\n",
    "                \"type\": \"text\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = Index(\"docs\", settings_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85fa74b3aadb4a5eaf56c074332bbead",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200001), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0:03:02.558291\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "index.add_documents(\"res/json\")\n",
    "elapsed = time.time() - start\n",
    "print(str(timedelta(seconds=elapsed)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-22-d24f33cd0290>, line 21)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-22-d24f33cd0290>\"\u001b[0;36m, line \u001b[0;32m21\u001b[0m\n\u001b[0;31m    def fulltext_query():\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class Query:\n",
    "    def __init__(self, task_id, query, relevant_docs):\n",
    "        self.task_id = task_id\n",
    "        self.query = query\n",
    "        self.relevant_docs = relevant_docs\n",
    "\n",
    "    def json_query(self):\n",
    "        return {\n",
    "    'query': {\n",
    "        'bool': {\n",
    "            'should': [\n",
    "                {\n",
    "                    'match': {\n",
    "                        'text': self.query\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "            \n",
    "    def fulltext_query():\n",
    "         return   {\n",
    "    'query': {\n",
    "        'bool': {\n",
    "            'should': [\n",
    "                {\n",
    "                    'match': {\n",
    "                        'text': self.query\n",
    "                    }\n",
    "                }\n",
    "                {\n",
    "                    'rank_feature': {\n",
    "                        'field': 'pagerank',\n",
    "                        'saturation': {\n",
    "                            'pivot': 10\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchQualityChecker:\n",
    "    def __init__(self, queries, index):\n",
    "        self.queries = queries\n",
    "        self.index = index\n",
    "        self.results = {}\n",
    "        \n",
    "    def get_results(self):\n",
    "        r_precision_total = 0\n",
    "        map_score_total = 0\n",
    "        r_total = 0\n",
    "        p_total = 0\n",
    "        for q in tqdm(self.queries):\n",
    "            res = self.index.search(q.json_query())\n",
    "            scores = get_score(res)\n",
    "            p_total += self.p(20, q, scores)\n",
    "            r_total += self.r(20, q, scores)\n",
    "            r_precision_total += self.r_precision(q, scores)\n",
    "            map_score_total += self.map_score(q, scores, 20)\n",
    "        Q = len(self.queries)\n",
    "        return r_precision_total / Q, map_score_total / Q, p_total / Q, r_total / Q\n",
    "    \n",
    "    def r_precision(self, query, search_res_score):\n",
    "        return self.r(len(query.relevant_docs), query, search_res_score)\n",
    "    \n",
    "    def map_score(self, query, search_res_score, n):\n",
    "        m = 0\n",
    "        for k in range(1, n):       \n",
    "            m += self.p(k, query, search_res_score)\n",
    "        R = len(query.relevant_docs)\n",
    "        return m / n\n",
    "    \n",
    "    def p(self, k, query, search_res_score):\n",
    "        r = 0\n",
    "        for doc, _ in search_res_score[:k]:\n",
    "            if doc in query.relevant_docs:\n",
    "                r += 1\n",
    "        return r / k\n",
    "    \n",
    "    def r(self, k, query, search_res_score):\n",
    "        R = len(query.relevant_docs)\n",
    "        r = 0\n",
    "        for doc, _ in search_res_score[:k]:\n",
    "            if doc in query.relevant_docs:\n",
    "                r += 1\n",
    "        return r / R if R != 0 else 0 if len(search_res_score) > 0 else 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevance():\n",
    "    res = {}\n",
    "    xml_tree = etree.parse(\"data/or_relevant-minus_table.xml\")\n",
    "    root = xml_tree.getroot()\n",
    "    for task in root.getchildren():\n",
    "        relevant_docs = set()\n",
    "        for document in task.getchildren():\n",
    "            if document.get(\"relevance\") == \"vital\":\n",
    "                relevant_docs.add(document.get(\"id\"))\n",
    "        res[task.get(\"id\")] = relevant_docs\n",
    "    print(len(res))\n",
    "    return res\n",
    "\n",
    "\n",
    "def generate_queries_plain_texts():\n",
    "    relevances = get_relevance()\n",
    "    xml_tree = etree.parse(\"data/web2008_adhoc.xml\")\n",
    "    root = xml_tree.getroot()\n",
    "    res = []\n",
    "    for task in root.getchildren():\n",
    "        if task.get(\"id\") is not None:\n",
    "            for query_text in task.getchildren():\n",
    "                try:\n",
    "                    res.append(Query(task.get(\"id\"), query_text.text, relevances[task.get(\"id\")]))\n",
    "                except:\n",
    "                    pass\n",
    "    print(len(res))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = generate_queries_plain_texts()\n",
    "quality_checker = SearchQualityChecker(queries, index)\n",
    "plain_text_res = quality_checker.get_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plain_text_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "russian_stopwords = stopwords.words(\"russian\")\n",
    "english_stopwords = stopwords.words(\"english\")\n",
    "black_list = [\"°\", \"№\", \"©\", \"...\", \"//\", \"://\", \"</\", \"\\\">\", \"=\\\"\", \"=\\'\", \"\\r\", \"\\n\", \"\\t\"]\n",
    "stem = Mystem()\n",
    "\n",
    "def lemmatize(text):\n",
    "    words = nltk.word_tokenize(text.lower())\n",
    "    tokens = []\n",
    "    for word in words:\n",
    "        tokens.extend(stem.lemmatize(word))\n",
    "    tokens = [token for token in tokens if token != \" \" and token.strip() not in punctuation \\\n",
    "              and token not in russian_stopwords and token not in english_stopwords \\\n",
    "              and token not in black_list \\\n",
    "              and token.find(\"\\r\") == -1 \\\n",
    "              and token.find(\"\\n\") == -1 \\\n",
    "              and token.find(\"\\t\") == -1 \\\n",
    "              and not (token.isdigit() and len(token) == 1)]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def generate_queries_lemmas():\n",
    "    relevances = get_relevance()\n",
    "    xml_tree = etree.parse(\"data/web2008_adhoc.xml\")\n",
    "    root = xml_tree.getroot()\n",
    "    res = []\n",
    "    for task in tqdm(root.getchildren()):\n",
    "        if task.get(\"id\") is not None:\n",
    "            for query_text in task.getchildren():\n",
    "                try:\n",
    "                    res.append(Query(task.get(\"id\"), lemmatize(query_text.text), relevances[task.get(\"id\")]))\n",
    "                except:\n",
    "                    pass\n",
    "    print(len(res))\n",
    "    return res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_lemmas = generate_queries_lemmas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_lemmas[0].query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_index = Index(\"lemma_docs\", settings_1)\n",
    "\n",
    "start = time.time()\n",
    "lemma_index.add_documents(\"data/json_filtered_tokens_texts\")\n",
    "elapsed = time.time() - start\n",
    "print(str(timedelta(seconds=elapsed)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_quality_checker = SearchQualityChecker(queries_lemmas, lemma_index)\n",
    "lemma_res = lemma_quality_checker.get_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_pagerank = {}\n",
    "with open('res/pagerank.txt','r') as f:\n",
    "    for line in f:\n",
    "        docId, docURL, rank = line.split()\n",
    "        id_to_pagerank[int(docId)] = float(rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "199202"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(id_to_pagerank)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92647fca46594e57ab3a7d3ea6c35008",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    " for doc_name in tqdm(os.listdir(\"data/json_filtered_tokens_texts\")):\n",
    "        with open(f\"data/json_filtered_tokens_texts/{doc_name}\", \"r+\", encoding=\"utf-8\") as inf:\n",
    "            doc_id = int(''.join(list(filter(str.isdigit, doc_name))))\n",
    "            doc = json.load(inf)\n",
    "            try:\n",
    "                doc[\"pagerank\"] = id_to_pagerank.get(doc_id)\n",
    "            except:\n",
    "                pass\n",
    "            inf.seek(0)        # <--- should reset file position to the beginning.\n",
    "            json.dump(doc, inf, indent=4, ensure_ascii=False)\n",
    "            inf.truncate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings_with_pagerank = {\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"text\": {\n",
    "                \"type\": \"text\"\n",
    "            },\n",
    "            \"pagerank\": {\n",
    "                \"type\": \"rank_feature\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_index = Index(\"pagerank_index\", settings_with_pagerank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fa893568e2d4f46bfaefd5521837b1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "('3 document(s) failed to index.', [{'index': {'_index': 'pagerank_index', '_type': '_doc', '_id': '1204092', 'status': 400, 'error': {'type': 'mapper_parsing_exception', 'reason': \"failed to parse field [pagerank] of type [rank_feature] in document with id '1204092'. Preview of field's value: '0'\", 'caused_by': {'type': 'illegal_argument_exception', 'reason': 'featureValue must be a positive normal float, got: 0.0for feature pagerank on field _feature which is less than the minimum positive normal float: 1.17549435E-38'}}, 'data': {'text': 'english version услуга продукт решение партнер новость компания группа iba контакт поиск главный новость добро пожаловать www kancler смотреть также подписываться наш новость добро пожаловать www kancler пакет прикладной программа ппп канцлер это новое поколение программный продукт наш компания платформа lotus domino notes предназначать создание система электронный документооборот сэд орган государственный управление крупный территориально распределенный организация производственный предприятие банк специализированный информационный ресурс www kancler разрабатывать наш студия веб дизайн pixelhead оперативный информирование потенциальный заказчик партнер ппп канцлер возможность особенность условие поставка также услуга наш компания часть создание система электронный документооборот посетитель сайт находить ответ многие вопрос электронный документооборот электронный документооборот организация нужный выбирать система электронный документооборот представитель организация заинтересованный создание сэд находиться поиск подходящий программный продукт получать ответ следующий вопрос ппп канцлер функциональный возможность предоставляться результат ожидать внедрение каков структура технологический особенность реализация отличаться программный продукт создание сэд сколько стоять условие поставляться технический требование предъявляться клиентский серверный часть происходить внедрение продукт сайт представлять информация ппп канцлер результат проект наш компания создание система электронный документооборот производственный предприятие орган государственный управление также отзыв заказчик наш награда необходимость посмотреть загружать презентация информационный листовка задавать вопрос подписываться новость добро пожаловать www kancler сайт посвящать новый программный продукт iba электронный документооборот дата последний модификация 20 октябрь 2005 версия печать индекс сайт право защищать 1993 2007 jv iba', 'pagerank': 0}}}, {'index': {'_index': 'pagerank_index', '_type': '_doc', '_id': '338948', 'status': 400, 'error': {'type': 'mapper_parsing_exception', 'reason': \"failed to parse field [pagerank] of type [rank_feature] in document with id '338948'. Preview of field's value: '0'\", 'caused_by': {'type': 'illegal_argument_exception', 'reason': 'featureValue must be a positive normal float, got: 0.0for feature pagerank on field _feature which is less than the minimum positive normal float: 1.17549435E-38'}}, 'data': {'text': 'читать подавать подавать объявление телеэкран реклама контакт весь страна брест витебск гомель гродно минск могилев недвижимость аренда квартира снимать доска объявление недвижимость аренда квартира снимать семья снимать комнатный квартира телефон платный размещение объявление 66 семья снимать комнатный квартира семья снимать комнатный квартира желательно фрунзенский район т 255 96 55 город минск телефон 255 96 55 добавлять 15.09 2006 12 03 заголовок семья снимать комнатный квартира отправлять объявление e mail добавлять закладка читать подавать подавать объявление телеэкран реклама контакт copyright ра сие сие 2006', 'pagerank': 0}}}, {'index': {'_index': 'pagerank_index', '_type': '_doc', '_id': '326209', 'status': 400, 'error': {'type': 'mapper_parsing_exception', 'reason': \"failed to parse field [pagerank] of type [rank_feature] in document with id '326209'. Preview of field's value: '0'\", 'caused_by': {'type': 'illegal_argument_exception', 'reason': 'featureValue must be a positive normal float, got: 0.0for feature pagerank on field _feature which is less than the minimum positive normal float: 1.17549435E-38'}}, 'data': {'text': 'читать подавать подавать объявление телеэкран реклама контакт весь страна брест витебск гомель гродно минск могилев услуга образование помощь учеба доска объявление услуга образование помощь учеба делопроизводство телефон платный размещение объявление 66 делопроизводство контрольный работа оформление компьютер распечатка быстро качественно недорого марина город г минск телефон 029 507 10 08 e mail добавлять 22.10 2006 12 20 заголовок делопроизводство отправлять объявление e mail добавлять закладка читать подавать подавать объявление телеэкран реклама контакт copyright ра сие сие 2006', 'pagerank': 0}}}])\n"
     ]
    }
   ],
   "source": [
    "pr_index.add_documents(\"data/json_filtered_tokens_texts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

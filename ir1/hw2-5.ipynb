{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import parallel_bulk\n",
    "import os\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import time\n",
    "from lxml import etree\n",
    "from sklearn.metrics import r2_score\n",
    "from datetime import timedelta\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_es_action(index, doc_id, document):\n",
    "    return {\n",
    "        '_index': index,\n",
    "        '_id': doc_id,\n",
    "        '_source': document\n",
    "    }\n",
    "\n",
    "def pretty_print_result(search_result, fields=None):\n",
    "    if fields is None:\n",
    "        fields = []\n",
    "    res = search_result['hits']\n",
    "    print(f'Total documents: {res[\"total\"][\"value\"]}')\n",
    "    for hit in res['hits']:\n",
    "        print(f'Doc {hit[\"_id\"]}, score is {hit[\"_score\"]}')\n",
    "        for field in fields:\n",
    "            print(f'{field}: {hit[\"_source\"][field]}')\n",
    "\n",
    "\n",
    "def get_score(search_result):\n",
    "    res = []\n",
    "    for hit in search_result['hits']['hits']:\n",
    "        res.append((hit[\"_id\"], hit[\"_score\"]))\n",
    "    res.sort(key = lambda x: x[1], reverse = True)\n",
    "    return res\n",
    "\n",
    "\n",
    "class Index:\n",
    "    def __init__(self, index, settings):\n",
    "        self.index_name = index\n",
    "        self.settings = settings\n",
    "        self.es = Elasticsearch([{'host': 'localhost', 'port': 9200, 'timeout': 360}])\n",
    "        if self.es.indices.exists(index=index):\n",
    "            self.es.indices.delete(index=index)\n",
    "        self.es.indices.create(index=index, body=settings)\n",
    "\n",
    "    def es_actions_generator(self, path_to_docs):\n",
    "        for doc_name in tqdm(os.listdir(path_to_docs)):\n",
    "            with open(f\"{path_to_docs}/{doc_name}\", \"r\", encoding=\"utf-8\") as inf:\n",
    "                doc_id = int(''.join(list(filter(str.isdigit, doc_name))))\n",
    "                doc = json.load(inf)           \n",
    "            yield create_es_action(self.index_name, doc_id, doc)\n",
    "\n",
    "\n",
    "    def add_documents(self, path_to_docs):\n",
    "        try:\n",
    "            for ok, result in parallel_bulk(self.es, self.es_actions_generator(path_to_docs), queue_size=4, thread_count=4,\n",
    "                                        chunk_size=1000):\n",
    "                  if not ok:\n",
    "                     print(result)\n",
    "        except Exception as e: \n",
    "            print(e)\n",
    "\n",
    "    def get_doc_by_id(self, doc_id):\n",
    "        return self.es.get(index=self.index_name, id=doc_id)['_source']\n",
    "\n",
    "    def search(self, query, *args):\n",
    "        return self.es.search(index=self.index_name, body=query, size=20)\n",
    "        # note that size set to 20 just because default value is 10 and we know that we have 12 docs and 10 < 12 < 20\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_index_size(index): \n",
    "    print(f\"{(index.es.indices.stats(index.index_name)['_all']['primaries']['store']['size_in_bytes'] / 2 ** 30):.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings_1 = {\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"text\": {\n",
    "                \"type\": \"text\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = {\n",
    "        'mappings': {\n",
    "            'properties': {\n",
    "                'text': {\n",
    "                    'type': 'text',\n",
    "                    'analyzer': 'russian_complex',\n",
    "                    'search_analyzer': 'russian_complex'\n",
    "                }\n",
    "            }\n",
    "            \n",
    "        },\n",
    "        \"settings\": {\n",
    "        \"analysis\" : {\n",
    "            \"analyzer\" : {\n",
    "                \"my_analyzer\" : {\n",
    "                    \"tokenizer\" : \"standard\",\n",
    "                    \"filter\" : [\"lowercase\", \"russian_snow\", \"english_snow\"]\n",
    "                },\n",
    "                'russian_complex': {\n",
    "                    'char_filter': [\n",
    "                        'yont'\n",
    "                    ],\n",
    "                    'tokenizer': 'word_longer_2',\n",
    "                    'filter': [\n",
    "                        'lowercase',\n",
    "                        'russian_snow'\n",
    "                    ]\n",
    "                }\n",
    "            },\n",
    "            'char_filter': {\n",
    "                'yont': {\n",
    "                    'type': 'mapping',\n",
    "                    'mappings': [\n",
    "                        'ё => е'\n",
    "                    ]\n",
    "                }\n",
    "            },\n",
    "            'tokenizer': {\n",
    "                'word_longer_2': {\n",
    "                    'type': 'pattern',\n",
    "                    'pattern': '[a-zA-Z_0-9\\u0400-\\u04FF]{2,}',\n",
    "                    'group': 0\n",
    "                },\n",
    "                'white_20': {\n",
    "                    'type': 'whitespace',\n",
    "                    'max_token_length': 5\n",
    "                }\n",
    "            },\n",
    "            \"filter\" : {\n",
    "                \"russian_snow\" : {\n",
    "                    \"type\" : \"snowball\",\n",
    "                    \"language\" : \"Russian\"\n",
    "                },\n",
    "                \"english_snow\" : {\n",
    "                    \"type\" : \"snowball\",\n",
    "                    \"language\" : \"English\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = Index(\"docs\", settings_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "index.add_documents(\"data/json_text\")\n",
    "elapsed = time.time() - start\n",
    "print(str(timedelta(seconds=elapsed)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_index_size(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_index = Index('stem_docs', settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "stem_index.add_documents(\"res/stemmed_titles\")\n",
    "elapsed = time.time() - start\n",
    "print(str(timedelta(seconds=elapsed)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_index_size(stem_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_index.get_doc_by_id('1000039')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Query:\n",
    "    def __init__(self, task_id, query, relevant_docs):\n",
    "        self.task_id = task_id\n",
    "        self.query = query\n",
    "        self.relevant_docs = relevant_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def json_query(query):\n",
    "    return {\n",
    "    'query': {\n",
    "        'bool': {\n",
    "            'should': [\n",
    "                {\n",
    "                    'match': {\n",
    "                        'text': query.query\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pagerank_query(query):\n",
    "    return  {\n",
    "    'query': {\n",
    "        'bool': {\n",
    "            'should': [\n",
    "                {\n",
    "                    'match': {\n",
    "                        'content': query.query\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    'match': {\n",
    "                        'title': {\n",
    "                            'query': query.query,\n",
    "                            'boost': '0.15'\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    'rank_feature': {\n",
    "                        'field': 'pagerank',\n",
    "                        'saturation': {\n",
    "                            'pivot': 10\n",
    "                        },\n",
    "                        'boost': '0.15'\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_query(query):\n",
    "    return  {\n",
    "    'query': {\n",
    "        'bool': {\n",
    "            'should': [\n",
    "                {\n",
    "                    'match': {\n",
    "                        'content': query.query\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    'match': {\n",
    "                        'title': {\n",
    "                            'query': query.query,\n",
    "                            'boost': '0.15'\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    'rank_feature': {\n",
    "                        'field': 'pagerank',\n",
    "                        'saturation': {\n",
    "                            'pivot': 10\n",
    "                        },\n",
    "                        'boost': '5'\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    'rank_feature': {\n",
    "                        'field': 'urllen',\n",
    "                        'boost': '0.5'\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    'rank_feature': {\n",
    "                        'field': 'doclen',\n",
    "                        'boost': '0.15'\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics:\n",
    "    def __init__(self, p, r, r_precision, map_score):\n",
    "        self.r = r\n",
    "        self.p = p\n",
    "        self.r_precision = r_precision\n",
    "        self.map_score = map_score\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"r = {self.r}\\np = {self.p}\\nr_precision = {self.r_precision}\\nMAP = {self.map_score}\"\n",
    "    \n",
    "    __repr__ = __str__\n",
    "\n",
    "\n",
    "class SearchQualityChecker:\n",
    "    def __init__(self, queries, index):\n",
    "        self.queries = queries\n",
    "        self.index = index\n",
    "        self.results = {}\n",
    "        self.metrics = {}\n",
    "        \n",
    "    def get_results(self, get_query=json_query):\n",
    "        r_precision_total = 0\n",
    "        map_score_total = 0\n",
    "        r_total = 0\n",
    "        p_total = 0\n",
    "        for q in tqdm(self.queries):\n",
    "            res = self.index.search(get_query(q))\n",
    "            print(q.task_id)\n",
    "            pretty_print_result(res)\n",
    "            scores = get_score(res)\n",
    "            metric = Metrics(p=self.p(20, q, scores), r=self.r(20, q, scores), r_precision=self.r_precision(q, scores),\n",
    "                            map_score=self.map_score(q, scores, 20))\n",
    "            p_total += metric.p\n",
    "            r_total += metric.r\n",
    "            r_precision_total += metric.r_precision\n",
    "            map_score_total += metric.map_score\n",
    "            self.metrics[q.task_id] = metric\n",
    "        Q = len(self.queries)\n",
    "        print(Q)\n",
    "        return Metrics(p=p_total / Q, r=r_total / Q, r_precision=r_precision_total / Q, map_score=map_score_total / Q)\n",
    "    \n",
    "    def r_precision(self, query, search_res_score):\n",
    "        return self.r(len(query.relevant_docs), query, search_res_score)\n",
    "    \n",
    "    def map_score(self, query, search_res_score, n):\n",
    "        m = 0\n",
    "        for k in range(1, n):       \n",
    "            m += self.p(k, query, search_res_score)\n",
    "        R = len(query.relevant_docs)\n",
    "        return m / n\n",
    "    \n",
    "    def p(self, k, query, search_res_score):\n",
    "        r = 0\n",
    "        for doc, _ in search_res_score[:k]:\n",
    "            if doc in query.relevant_docs:\n",
    "                r += 1\n",
    "        return r / k\n",
    "    \n",
    "    def r(self, k, query, search_res_score):\n",
    "        R = len(query.relevant_docs)\n",
    "        r = 0\n",
    "        for doc, _ in search_res_score[:k]:\n",
    "            if doc in query.relevant_docs:\n",
    "                r += 1\n",
    "        return r / R if R != 0 else 0 if len(search_res_score) > 0 else 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_diff_metrics(quality_checker, other_checker, k=20, comp=lambda x : x.map_score):\n",
    "     res = []\n",
    "     for task_id in quality_checker.metrics:\n",
    "        metric = quality_checker.metrics[task_id]\n",
    "        other_metric = other_checker.metrics[task_id]\n",
    "        res.append((task_id, abs(comp(metric) - comp(other_metric)), metric, other_metric))\n",
    "     res.sort(reverse=True, key=lambda x: x[1])\n",
    "     return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def get_relevance():\n",
    "    res = defaultdict(list)\n",
    "    xml_tree = etree.parse(\"data/relevant_table_2009.xml\")\n",
    "    root = xml_tree.getroot()\n",
    "    for task in root.getchildren():\n",
    "        relevant_docs = set()\n",
    "        for document in task.getchildren():\n",
    "            if document.get(\"relevance\") == \"vital\":\n",
    "                relevant_docs.add(document.get(\"id\"))\n",
    "        res[task.get(\"id\")] = relevant_docs\n",
    "    print(len(res))\n",
    "    return res\n",
    "\n",
    "\n",
    "def generate_queries_plain_texts():\n",
    "    relevances = get_relevance()\n",
    "    xml_tree = etree.parse(\"data/web2008_adhoc.xml\")\n",
    "    root = xml_tree.getroot()\n",
    "    res = []\n",
    "    for task in root.getchildren():\n",
    "        if task.get(\"id\") is not None:\n",
    "            for query_text in task.getchildren():\n",
    "                try:\n",
    "                    if len(relevances[task.get(\"id\")]) > 0:\n",
    "                        res.append(Query(task.get(\"id\"), query_text.text, relevances[task.get(\"id\")]))\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "    print(len(res))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = generate_queries_plain_texts()\n",
    "#print(queries)\n",
    "quality_checker = SearchQualityChecker(queries, index)\n",
    "plain_text_res = quality_checker.get_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plain_text_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import re\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.snowball import RussianStemmer\n",
    "\n",
    "russian_stopwords = stopwords.words(\"russian\")\n",
    "english_stopwords = stopwords.words(\"english\")\n",
    "black_list = [\"°\", \"№\", \"©\", \"...\", \"//\", \"://\", \"</\", \"\\\">\", \"=\\\"\", \"=\\'\", \"\\r\", \"\\n\", \"\\t\"]\n",
    "stem = Mystem()\n",
    "\n",
    "def lemmatize(text):\n",
    "    words = nltk.word_tokenize(text.lower())\n",
    "    tokens = []\n",
    "    for word in words:\n",
    "        tokens.extend(stem.lemmatize(word))\n",
    "    tokens = [token for token in tokens if token != \" \" and token.strip() not in punctuation \\\n",
    "              and token not in russian_stopwords and token not in english_stopwords \\\n",
    "              and token not in black_list \\\n",
    "              and token.find(\"\\r\") == -1 \\\n",
    "              and token.find(\"\\n\") == -1 \\\n",
    "              and token.find(\"\\t\") == -1 \\\n",
    "              and not (token.isdigit() and len(token) == 1)]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def stemmize(text):\n",
    "    english_check = re.compile(r'[a-z]')\n",
    "    stemmerr = RussianStemmer()\n",
    "    stemmere = PorterStemmer()\n",
    "    \n",
    "    lowercase_text = text.lower()\n",
    "\n",
    "    tokens = []\n",
    "    for token in word_tokenize(lowercase_text):\n",
    "        if english_check.match(token):\n",
    "            tokens.append(stemmere.stem(token))\n",
    "        else:\n",
    "            tokens.append(stemmerr.stem(token))\n",
    "\n",
    "    tokens = [token for token in tokens if token != \" \" and token.strip() not in punctuation \\\n",
    "              and token not in russian_stopwords and token not in english_stopwords]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def generate_queries_lemmas():\n",
    "    relevances = get_relevance()\n",
    "    xml_tree = etree.parse(\"data/web2008_adhoc.xml\")\n",
    "    root = xml_tree.getroot()\n",
    "    res = []\n",
    "    for task in tqdm(root.getchildren()):\n",
    "        if task.get(\"id\") is not None:\n",
    "            for query_text in task.getchildren():\n",
    "                try:\n",
    "                    if len(relevances[task.get(\"id\")]) > 0:\n",
    "                        res.append(Query(task.get(\"id\"), lemmatize(query_text.text), relevances[task.get(\"id\")]))\n",
    "                except:\n",
    "                    pass\n",
    "    print(len(res))\n",
    "    return res\n",
    "\n",
    "def generate_queries_stem():\n",
    "    relevances = get_relevance()\n",
    "    xml_tree = etree.parse(\"data/web2008_adhoc.xml\")\n",
    "    root = xml_tree.getroot()\n",
    "    res = []\n",
    "    for task in tqdm(root.getchildren()):\n",
    "        if task.get(\"id\") is not None:\n",
    "            for query_text in task.getchildren():\n",
    "                try:\n",
    "                    res.append(Query(task.get(\"id\"), stemmize(query_text.text), relevances[task.get(\"id\")]))\n",
    "                except:\n",
    "                    pass\n",
    "    print(len(res))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_lemmas = generate_queries_lemmas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_lemmas[8].query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_index = Index(\"lemma_docs\", settings_1)\n",
    "\n",
    "start = time.time()\n",
    "lemma_index.add_documents(\"data/json_filtered_tokens_texts\")\n",
    "elapsed = time.time() - start\n",
    "print(str(timedelta(seconds=elapsed)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_quality_checker = SearchQualityChecker(queries_lemmas, lemma_index)\n",
    "lemma_res = lemma_quality_checker.get_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_index_size(lemma_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_diff_metrics(quality_checker, lemma_quality_checker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_stem = generate_queries_stem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_pagerank = {}\n",
    "with open('data/pagerank.txt','r') as f:\n",
    "    for line in f:\n",
    "        docId, docURL, rank = line.split()\n",
    "        id_to_pagerank[int(docId)] = float(rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(id_to_pagerank)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " for doc_name in tqdm(os.listdir(\"data/json_filtered_tokens_texts\")):\n",
    "        with open(f\"data/json_filtered_tokens_texts/{doc_name}\", \"r+\", encoding=\"utf-8\") as inf:\n",
    "            doc_id = int(''.join(list(filter(str.isdigit, doc_name))))\n",
    "            doc = json.load(inf)\n",
    "            try:\n",
    "                doc[\"pagerank\"] = id_to_pagerank.get(doc_id)\n",
    "            except:\n",
    "                pass\n",
    "            inf.seek(0)        # <--- should reset file position to the beginning.\n",
    "            json.dump(doc, inf, indent=4, ensure_ascii=False)\n",
    "            inf.truncate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings_with_pagerank = {\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"title\": {\n",
    "                \"type\": \"text\"\n",
    "            },\n",
    "            \"content\": {\n",
    "                \"type\": \"text\"\n",
    "            },\n",
    "            \"pagerank\": {\n",
    "                \"type\": \"rank_feature\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_index = Index(\"pagerank_index\", settings_with_pagerank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_index.add_documents(\"res/lemmatized_titles_pr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_index_size(pr_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_quality_checker = SearchQualityChecker(queries_lemmas, pr_index)\n",
    "pr_res = pr_quality_checker.get_results(pagerank_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_index.get_doc_by_id(1000039)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings_titles = {\n",
    "        'mappings': {\n",
    "            'properties': {\n",
    "                'content': {\n",
    "                    'type': 'text',\n",
    "                },\n",
    "                'title': {\n",
    "                    'type': 'text'\n",
    "                }\n",
    "            }\n",
    "            \n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_quality_checker = SearchQualityChecker(queries_lemmas, pr_index)\n",
    "pr_res = pr_quality_checker.get_results(pagerank_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings_all = {\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"title\": {\n",
    "                \"type\": \"text\"\n",
    "            },\n",
    "            \"content\": {\n",
    "                \"type\": \"text\"\n",
    "            },\n",
    "            \"urllen\": {\n",
    "                \"type\": \"rank_feature\"\n",
    "            },\n",
    "            \"doclen\": {\n",
    "                \"type\": \"rank_feature\"\n",
    "            },\n",
    "            \"pagerank\": {\n",
    "                \"type\": \"rank_feature\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_index = Index(\"all_index\", settings_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_index.add_documents(\"res/lemmatized_titles_pr_len\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_index_size(all_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_quality_checker = SearchQualityChecker(queries_lemmas, all_index)\n",
    "all_res = all_quality_checker.get_results(all_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_query_creator(b1, b2, b3, b4):\n",
    "    def all_query_in(query):\n",
    "        return  {\n",
    "        'query': {\n",
    "            'bool': {\n",
    "                'should': [\n",
    "                    {\n",
    "                        'match': {\n",
    "                            'content': query.query\n",
    "                        }\n",
    "                    },\n",
    "                    {\n",
    "                        'match': {\n",
    "                            'title': {\n",
    "                                'query': query.query,\n",
    "                                'boost': str(b1)\n",
    "                            }\n",
    "                        }\n",
    "                    },\n",
    "                    {\n",
    "                        'rank_feature': {\n",
    "                            'field': 'pagerank',\n",
    "                            'saturation': {\n",
    "                                'pivot': 10\n",
    "                            },\n",
    "                            'boost': str(b2)\n",
    "                        }\n",
    "                    },\n",
    "                    {\n",
    "                        'rank_feature': {\n",
    "                            'field': 'urllen',\n",
    "                            'boost': str(b3)\n",
    "                    }\n",
    "                    },\n",
    "                    {\n",
    "                        'rank_feature': {\n",
    "                            'field': 'doclen',\n",
    "                            'boost': str(b4)\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return all_query_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ress = []\n",
    "for b1 in [0.15, 0.3, 0.8, 1.5, 3, 5]:\n",
    "    for b2 in [0.15, 0.3, 0.8, 1.5, 3, 5]:\n",
    "        for b3 in [0.15, 0.3, 0.8, 1.5, 3, 5]:\n",
    "            for b4 in [0.15, 0.3, 0.8, 1.5, 3, 5]:\n",
    "                all_res = all_quality_checker.get_results(all_query_creator(b1, b2, b3, b4))\n",
    "                all_ress.append(all_res)\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db1=0.15\n",
    "db2=5.0\n",
    "db3=0.5\n",
    "db4=0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ress1 = []\n",
    "r =[]\n",
    "p = []\n",
    "rp = []\n",
    "mapp = []\n",
    "#b1l = [0.02, 0.05, 0.08, 0.11, 0.14, 0.17, 0.2, 0.23, 0.26, 0.29]\n",
    "b1l = [0.2]\n",
    "for b1 in b1l:\n",
    "    all_res = all_quality_checker.get_results(all_query_creator(b1, db2, db3, db4))\n",
    "    a, b, c, d = all_res.p, all_res.r, all_res.r_precision, all_res.map_score\n",
    "    r.append(a)\n",
    "    p.append(b)\n",
    "    rp.append(c)\n",
    "    mapp.append(d)\n",
    "    all_ress.append(all_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.clf()\n",
    "plt.xlabel('Boost')\n",
    "plt.ylabel('r@20')\n",
    "plt.plot(b1l, r)\n",
    "plt.savefig(\"res/plots/r11.png\")\n",
    "\n",
    "plt.clf()\n",
    "plt.xlabel('Boost')\n",
    "plt.ylabel('p@20')\n",
    "plt.plot(b1l, p)\n",
    "plt.savefig(\"res/plots/p11.png\")\n",
    "\n",
    "plt.clf()\n",
    "plt.xlabel('Boost')\n",
    "plt.ylabel('r_prec')\n",
    "plt.plot(b1l, rp)\n",
    "plt.savefig(\"res/plots/rp11.png\")\n",
    "\n",
    "plt.clf()\n",
    "plt.xlabel('Boost')\n",
    "plt.ylabel('map')\n",
    "plt.plot(b1l, mapp)\n",
    "plt.savefig(\"res/plots/map11.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ress1 = []\n",
    "r =[]\n",
    "p = []\n",
    "rp = []\n",
    "mapp = []\n",
    "b1l = [0.01, 0.15, 0.8, 1.5, 5]\n",
    "for b3 in b1l:\n",
    "    all_res = all_quality_checker.get_results(all_query_creator(db1, db2, b3, db4))\n",
    "    a, b, c, d = all_res.p, all_res.r, all_res.r_precision, all_res.map_score\n",
    "    r.append(a)\n",
    "    p.append(b)\n",
    "    rp.append(c)\n",
    "    mapp.append(d)\n",
    "    all_ress.append(all_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.clf()\n",
    "plt.xlabel('Boost')\n",
    "plt.ylabel('r@20')\n",
    "plt.plot(b1l, r)\n",
    "plt.savefig(\"res/plots/r2.png\")\n",
    "\n",
    "plt.clf()\n",
    "plt.xlabel('Boost')\n",
    "plt.ylabel('p@20')\n",
    "plt.plot(b1l, p)\n",
    "plt.savefig(\"res/plots/p2.png\")\n",
    "\n",
    "plt.clf()\n",
    "plt.xlabel('Boost')\n",
    "plt.ylabel('r_prec')\n",
    "plt.plot(b1l, rp)\n",
    "plt.savefig(\"res/plots/rp2.png\")\n",
    "\n",
    "plt.clf()\n",
    "plt.xlabel('Boost')\n",
    "plt.ylabel('map')\n",
    "plt.plot(b1l, mapp)\n",
    "plt.savefig(\"res/plots/map2.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ress1 = []\n",
    "r =[]\n",
    "p = []\n",
    "rp = []\n",
    "mapp = []\n",
    "b1l = [0.01, 0.15, 0.8, 1.5, 5]\n",
    "for b4 in b1l:\n",
    "    all_res = all_quality_checker.get_results(all_query_creator(db1, db2, db3, b4))\n",
    "    a, b, c, d = all_res.p, all_res.r, all_res.r_precision, all_res.map_score\n",
    "    r.append(a)\n",
    "    p.append(b)\n",
    "    rp.append(c)\n",
    "    mapp.append(d)\n",
    "    all_ress.append(all_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.clf()\n",
    "plt.xlabel('Boost')\n",
    "plt.ylabel('r@20')\n",
    "plt.plot(b1l, r)\n",
    "plt.savefig(\"res/plots/r3.png\")\n",
    "\n",
    "plt.clf()\n",
    "plt.xlabel('Boost')\n",
    "plt.ylabel('p@20')\n",
    "plt.plot(b1l, p)\n",
    "plt.savefig(\"res/plots/p3.png\")\n",
    "\n",
    "plt.clf()\n",
    "plt.xlabel('Boost')\n",
    "plt.ylabel('r_prec')\n",
    "plt.plot(b1l, rp)\n",
    "plt.savefig(\"res/plots/rp3.png\")\n",
    "\n",
    "plt.clf()\n",
    "plt.xlabel('Boost')\n",
    "plt.ylabel('map')\n",
    "plt.plot(b1l, mapp)\n",
    "plt.savefig(\"res/plots/map3.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
